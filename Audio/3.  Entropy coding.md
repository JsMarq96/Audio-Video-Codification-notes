Reducir la cantidad de bits en funcion de las estadisticas de usabilidad de cada simbolo: Un simpbolo usado mas, sera representado con menos bits.

## Probability
Probabilidad o cantidad que aparece un simbolo en la representacion final

## Information
La inversa de la probabilidad en escala logaritmica; es decir, cuanta mayor probabilidad, menor informacion necesita
$$ I(x_k)  = -\log_b (p(x_k)) $$

## Entropia
Informacion media de los simbolos en un alfabeto; y representa la cantidad media de bits por simbolo en un alfabeto para transmitir la informacion de manera optima. Ej: si nuestro codificado usa mas bits por simbolo que la entropia... mal vamos
$$ H(X) = - \sum_i^N p(x_i)\ \log_b(p(x_i)) $$
